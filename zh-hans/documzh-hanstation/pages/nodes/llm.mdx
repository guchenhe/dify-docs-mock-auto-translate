```mdx
---
title: "大型语言模型 (LLM)"
description: "Invoke language models for text generation and analysis"
icon: "brain"
---

LLM 节点调用大型语言模型来处理文本、图像和文档。它向您配置的模型发送提示词并捕获其响应，支持结构化输出、上下文管理和多模态输入。

<Frame caption="LLM 节点配置界面">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/85730fbfa1d441d12d969b89adf2670e.png" alt="LLM 节点概览" />
</Frame>

<Info>
  在使用 LLM 节点之前，请在 **System Settings → Model Providers** 中配置至少一个模型提供商。设置说明请参阅[模型配置指南](/en/guides/model-configuration/readme)。
</Info>

## 模型选择与参数

从您配置的任何模型提供商中选择。不同的模型擅长不同的任务 - GPT-4 和 Claude 3.5 能够很好地处理复杂推理但成本较高，而 GPT-3.5 Turbo 在性能和经济性之间取得了平衡。对于本地部署，请使用 Ollama、LocalAI 或 Xinference。

<Frame caption="模型选择与参数配置">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/43f81418ea70d4d79e3705505e777b1b.png" alt="LLM 节点配置" />
</Frame>

模型参数控制响应生成。**温度**范围从 0（确定性）到 1（创意性）。**核采样**通过概率限制词汇选择。**频率惩罚**减少重复。**存在惩罚**鼓励新主题。您还可以使用预设：**精确**、**平衡**或**创意**。

## 提示词配置

您的界面会根据模型类型进行调整。聊天模型使用消息角色（**System** 用于行为，**User** 用于输入，**Assistant** 用于示例），而完成模型使用简单的文本续写。

在提示词中使用双花括号引用工作流变量：`{{variable_name}}`。变量会在到达模型之前被实际值替换。

```text
System: 您是一位技术文档专家。
User: {{user_input}}
```

## 上下文变量

上下文变量在保留来源归属的同时注入外部知识。这使得通过检索增强生成 (RAG) 应用程序，LLM 能够使用您特定的文档回答问题。

<Frame caption="在 RAG 应用程序中使用上下文变量">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/5aefed96962bd994f8f05bac96b11e22.png" alt="上下文变量" />
</Frame>

将知识检索节点的输出连接到您的 LLM 节点的上下文输入，然后引用它：

```text
仅使用此上下文回答：
{{knowledge_retrieval.result}}

问题: {{user_question}}
```

使用知识检索的上下文变量时，Dify 会自动跟踪引用，以便用户查看信息来源。

## 结构化输出

强制模型返回特定的数据格式，如 JSON，以便程序使用。通过三种方法进行配置：

<Tabs>
  <Tab title="可视化编辑器">
    用户友好的界面用于简单结构。添加具有名称和类型的字段，标记必填字段，设置描述。编辑器会自动生成 JSON Schema。
  </Tab>
  
  <Tab title="JSON Schema">
    直接编写用于复杂结构的 Schema，包括嵌套对象、数组和验证规则。
    
    ```json
    {
      "type": "object",
      "properties": {
        "sentiment": {
          "type": "string",
          "enum": ["positive", "negative", "neutral"]
        }
      },
      "required": ["sentiment"]
    }
    ```
  </Tab>
  
  <Tab title="AI 生成">
    用简单语言描述需求，让 AI 生成 Schema。
  </Tab>
</Tabs>

<Warning>
  支持原生 JSON 的模型可以可靠地处理结构化输出。对于其他模型，Dify 会在提示词中包含 Schema，但结果可能有所不同。
</Warning>

## 记忆与文件处理

启用**记忆**以在工作流运行中的多个 LLM 调用间保持上下文。节点在后续提示词中包含先前的交互。记忆是节点特定的，并且不会在工作流运行之间持久化。

对于**文件处理**，将文件变量添加到多模态模型的提示词中。GPT-4V 处理图像，Claude 直接处理 PDF，而其他模型可能需要预处理。

### 视觉配置

处理图像时，您可以控制细节级别：
- **高细节** - 对复杂图像的准确性更高，但会使用更多标记
- **低细节** - 对简单图像处理更快，标记更少

视觉的默认变量选择器是 `sys.files`，它会自动从起始节点接收文件。

<Frame caption="使用多模态 LLM 进行文件处理">
  <img src="https://assets-docs.dify.ai/2024/11/05b3d4a78038bc7afbb157078e3b2b26.png" alt="文件处理" />
</Frame>

对于完成模型中的对话历史，将对话变量插入以保持多轮对话上下文：

<Frame caption="使用对话历史变量">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/b8642f8c6e3f562fceeefae83628fd68.png" alt="对话历史" />
</Frame>

## Jinja2 模板支持

LLM 提示词支持 Jinja2 模板以进行高级变量处理。当您使用 Jinja2 模式（`edition_type: "jinja2"`）时，您可以：

```jinja2
{% for item in search_results %}
{{ loop.index }}. {{ item.title }}: {{ item.content }}
{% endfor %}
```

Jinja2 变量与常规变量替换分开处理，允许在提示词中进行循环、条件和复杂的数据转换。

## 流式输出

LLM 节点默认支持流式输出。每个文本块作为 `RunStreamChunkEvent` 被传递，启用实时响应显示。文件输出（图像、文档）在流式传输期间自动处理并保存。

## 错误处理

配置失败的 LLM 调用的重试行为。设置最大重试次数、重试间隔和回退倍数。当重试不足时，定义备用策略，如默认值、错误路由或备用模型。
```