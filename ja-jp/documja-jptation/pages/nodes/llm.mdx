---
title: "LLM"
description: "Invoke language models for text generation and analysis"
icon: "brain"
---

LLMノードは、大規模言語モデルを呼び出してテキスト、画像、文書を処理します。設定されたモデルにプロンプトを送信し、その応答を取得します。構造化出力、コンテキスト管理、マルチモーダル入力をサポートしています。

<Frame caption="LLMノード設定インターフェース">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/85730fbfa1d441d12d969b89adf2670e.png" alt="LLM Node Overview" />
</Frame>

<Info>
  LLMノードを使用する前に、**システム設定 → モデルプロバイダー**で少なくとも1つのモデルプロバイダーを設定してください。セットアップ手順については[モデル設定ガイド](/en/guides/model-configuration/readme)をご覧ください。
</Info>

## モデル選択とパラメータ

設定済みの任意のモデルプロバイダーから選択できます。異なるモデルは異なるタスクに優れています。GPT-4とClaude 3.5は複雑な推論を上手く処理しますが、コストが高くなります。一方、GPT-3.5 Turboは性能と手頃な価格のバランスが取れています。ローカル配信には、Ollama、LocalAI、またはXinferenceを使用してください。

<Frame caption="モデル選択とパラメータ設定">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/43f81418ea70d4d79e3705505e777b1b.png" alt="LLM Node Configuration" />
</Frame>

モデルパラメータは応答生成を制御します。**温度**は0（決定論的）から1（創造的）の範囲です。**核サンプリング**は確率により単語選択を制限します。**頻度ペナルティ**は繰り返しを減らします。**存在ペナルティ**は新しいトピックを促します。プリセットも使用できます：**精密**、**バランス**、または**創造的**。

## プロンプト設定

インターフェースはモデルタイプに応じて適応されます。チャットモデルはメッセージ役割（**システム**は動作用、**ユーザー**は入力用、**アシスタント**は例用）を使用し、完了モデルは単純なテキスト継続を使用します。

プロンプト内でワークフロー変数を参照するには、二重中括弧を使用します：`{{variable_name}}`。変数はモデルに到達する前に実際の値に置き換えられます。

```text
System: あなたは技術文書の専門家です。
User: {{user_input}}
```

## コンテキスト変数

コンテキスト変数は、ソース帰属を保持しながら外部知識を注入します。これにより、大規模言語モデルが特定の文書を使用して質問に答えるRAGアプリケーションが可能になります。

<Frame caption="RAGアプリケーション用のコンテキスト変数の使用">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guidesed96962bd994f8f05bac96b11e22.png" alt="Context Variables" />
</Frame>

知識検索ノードの出力をLLMノードのコンテキスト入力に接続し、それを参照します：

```text
このコンテキストのみを使用して回答してください：
{{knowledge_retrieval.result}}

質問：{{user_question}}
```

知識検索からのコンテキスト変数を使用する場合、Difyは自動的に引用と帰属を追跡するため、ユーザーは情報源を確認できます。

## 構造化出力

プログラム用のJSONなど、特定のデータ形式を返すようモデルに強制します。3つの方法で設定できます：

<Tabs>
  <Tab title="ビジュアルエディター">
    シンプルな構造のためのユーザーフレンドリーなインターフェース。名前とタイプでフィールドを追加し、必須フィールドにマークし、説明を設定します。エディターは自動的にJSONスキーマを生成します。
  </Tab>
  
  <Tab title="JSON Schema">
    ネストされたオブジェクト、配列、検証ルールを持つ複雑な構造のために、スキーマを直接記述します。
    
    ```json
    {
      "type": "object",
      "properties": {
        "sentiment": {
          "type": "string",
          "enum": ["positive", "negative", "neutral"]
        }
      },
      "required": ["sentiment"]
    }
    ```
  </Tab>
  
  <Tab title="AI生成">
    平易な言葉でニーズを記述し、AIにスキーマを生成させます。
  </Tab>
</Tabs>

<Warning>
  ネイティブJSONサポートを持つモデルは構造化出力を確実に処理します。その他のモデルについては、Difyはプロンプトにスキーマを含めますが、結果は変動する場合があります。
</Warning>

## メモリとファイル処理

**メモリ**を有効にして、ワークフロー実行内の複数のLLM呼び出し間でコンテキストを維持します。ノードは以前のやり取りを後続のプロンプトに含めます。メモリはノード固有で、ワークフロー実行間では持続しません。

**ファイル処理**では、マルチモーダルモデル用のプロンプトにファイル変数を追加します。GPT-4Vは画像を処理し、ClaudeはPDFを直接処理しますが、他のモデルには前処理が必要な場合があります。

### ビジョン機能設定

画像を処理する際、詳細レベルを制御できます：
- **高詳細** - 複雑な画像に対してより良い精度を提供しますが、より多くの最大トークン数を使用します
- **低詳細** - シンプルな画像に対して少ない最大トークン数でより高速な処理を行います

ビジョン機能のデフォルト変数セレクタは`sys.files`で、開始ノードからファイルを自動的に取得します。

<Frame caption="マルチモーダル大規模言語モデルによるファイル処理">
  <img src="https://assets-docs.dify.ai/2024/11/05b3d4a78038bc7afbb157078e3b2b26.png" alt="File Processing" />
</Frame>

完了モデルでの会話履歴については、会話変数を挿入してマルチターンコンテキストを維持します：

<Frame caption="会話履歴変数の使用">
  <img src="https://assets-docs.dify.ai/dify-enterprise-mintlify/en/guides/workflow/node/b8642f8c6e3f562fceeefae83628fd68.png" alt="Conversation History" />
</Frame>

## Jinja2テンプレートサポート

LLMプロンプトは高度な変数処理のためのJinja2テンプレートをサポートします。Jinja2モード（`edition_type: "jinja2"`）を使用する場合：

```jinja2
{% for item in search_results %}
{{ loop.index }}. {{ item.title }}: {{ item.content }}
{% endfor %}常の変数置換とは別に処理され、プロンプト内でのループ、条件分岐、複雑なデータ変換が可能です。

## ストリーミングレスポンス

LLMノードはデフォルトでストリーミングレスポンスをサポートします。各テキストチャンクは`RunStreamChunkEvent`として出力され、リアルタイム応答表示を可能にします。ファイル出力（画像、文書）は、ストリーミング中に自動的に処理され保存されます。

## エラーハンドリング

失敗したLLM呼び出しの再試行動作を設定します。最大再試行回数、再試行間隔、バックオフ乗数を設定します。再試行が十分でない場合のデフォルト値、エラールーティング、代替モデルなどのフォールバック戦略を定義します。

{/*
Contributing Section
DO NOT edit this section!
It will be automatically generated by the script.
*/}

---

[このページを編集する](https://github.com/langgenius/dify-docs/edit/main/ja-jp/documja-jptation/pages/nodes/llm.mdx) | [問題を報告する](https://github.com/langgenius/dify-docs/issues/new?template=docs.yml)

